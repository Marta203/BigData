{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dcce73c",
   "metadata": {},
   "source": [
    "## 1. How to import PySpark and check the version?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a37d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Creating a SparkSession: A SparkSession is the entry point for using the PySpark DataFrame and SQL API.\n",
    "# To create a SparkSession, use the following code\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"PySpark 101 Exercises\").getOrCreate()\n",
    "\n",
    "# Get version details\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a945de07",
   "metadata": {},
   "source": [
    "## 2. How to convert the index of a PySpark DataFrame into a column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d07a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, monotonically_increasing_id\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "df = spark.createDataFrame([\n",
    "(\"Alice\", 1),\n",
    "(\"Bob\", 2),\n",
    "(\"Charlie\", 3),\n",
    "], [\"Name\", \"Value\"])\n",
    "\n",
    "# Define window specification\n",
    "w = Window.orderBy(monotonically_increasing_id())\n",
    "\n",
    "# Add index\n",
    "df = df.withColumn(\"index\", row_number().over(w) - 1)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c49522d",
   "metadata": {},
   "source": [
    "## 3. How to combine many lists to form a PySpark DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8589647",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = [\"a\", \"b\", \"c\", \"d\"]\n",
    "list2 = [1, 2, 3, 4]\n",
    "# Create an RDD from the lists and convert it to a DataFrame\n",
    "rdd = spark.sparkContext.parallelize(list(zip(list1, list2)))\n",
    "df = rdd.toDF([\"Column1\", \"Column2\"])\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36def377",
   "metadata": {},
   "source": [
    "## 4. How to get the items of list A not present in list B?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f7f323",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "\n",
    "list_A = [1, 2, 3, 4, 5]\n",
    "list_B = [4, 5, 6, 7, 8]\n",
    "# Convert lists to RDD\n",
    "rdd_A = sc.parallelize(list_A)\n",
    "rdd_B = sc.parallelize(list_B)\n",
    "\n",
    "# Perform subtract operation\n",
    "result_rdd = rdd_A.subtract(rdd_B)\n",
    "\n",
    "# Collect result\n",
    "result_list = result_rdd.collect()\n",
    "print(result_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b98b88a",
   "metadata": {},
   "source": [
    "## 5. How to get the items not common to both list A and list B?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc94fc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "\n",
    "list_A = [1, 2, 3, 4, 5]\n",
    "list_B = [4, 5, 6, 7, 8]\n",
    "\n",
    "# Convert lists to RDD\n",
    "rdd_A = sc.parallelize(list_A)\n",
    "rdd_B = sc.parallelize(list_B)\n",
    "\n",
    "# Perform subtract operation\n",
    "result_rdd_A = rdd_A.subtract(rdd_B)\n",
    "result_rdd_B = rdd_B.subtract(rdd_A)\n",
    "\n",
    "# Union the two RDDs\n",
    "result_rdd = result_rdd_A.union(result_rdd_B)\n",
    "\n",
    "# Collect result\n",
    "result_list = result_rdd.collect()\n",
    "\n",
    "print(result_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6d4e0b",
   "metadata": {},
   "source": [
    "## 6. How to get the minimum, 25th percentile, median, 75th, and max of a numeric column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dd9b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample DataFrame\n",
    "data = [(\"A\", 10), (\"B\", 20), (\"C\", 30), (\"D\", 40), (\"E\", 50), (\"F\", 15), (\"G\", 28), (\"H\", 54), (\"I\", 41), (\"J\", 86)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "# Calculate percentiles\n",
    "quantiles = df.approxQuantile(\"Age\", [0.0, 0.25, 0.5, 0.75, 1.0], 0.01)\n",
    "\n",
    "print(\"Min: \", quantiles[0])\n",
    "print(\"25th percentile: \", quantiles[1])\n",
    "print(\"Median: \", quantiles[2])\n",
    "print(\"75th percentile: \", quantiles[3])\n",
    "print(\"Max: \", quantiles[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08924e02",
   "metadata": {},
   "source": [
    "## 7. How to get frequency counts of unique items of a column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12de4234",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='Mary', job='Scientist'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Scientist'),\n",
    "Row(name='Sam', job='Doctor'),\n",
    "]\n",
    "\n",
    "# create DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# show DataFrame\n",
    "df.show()\n",
    "df.groupBy(\"job\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f7170a",
   "metadata": {},
   "source": [
    "## 8. How to keep only top 2 most frequent values as it is and replace everything else as ‘Other’?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a9d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='Mary', job='Scientist'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Scientist'),\n",
    "Row(name='Sam', job='Doctor'),\n",
    "]\n",
    "\n",
    "# create DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# show DataFrame\n",
    "df.show()\n",
    "\n",
    "# Get the top 2 most frequent jobs\n",
    "top_2_jobs = df.groupBy('job').count().orderBy('count', ascending=False).limit(2).select('job').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Replace all but the top 2 most frequent jobs with 'Other'\n",
    "df = df.withColumn('job', when(col('job').isin(top_2_jobs), col('job')).otherwise('Other'))\n",
    "\n",
    "# show DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563c89c2",
   "metadata": {},
   "source": [
    "## 9. How to Drop rows with NA values specific to a particular column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388f380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame\n",
    "df = spark.createDataFrame([\n",
    "(\"A\", 1, None),\n",
    "(\"B\", None, \"123\" ),\n",
    "(\"B\", 3, \"456\"),\n",
    "(\"D\", None, None),\n",
    "], [\"Name\", \"Value\", \"id\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "df_2 = df.dropna(subset=['Value'])\n",
    "\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31247401",
   "metadata": {},
   "source": [
    "## 10. How to rename columns of a PySpark DataFrame using two lists – one containing the old column names and the other containing the new column names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a11d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppose you have the following DataFrame\n",
    "df = spark.createDataFrame([(1, 2, 3), (4, 5, 6)], [\"col1\", \"col2\", \"col3\"])\n",
    "\n",
    "# old column names\n",
    "old_names = [\"col1\", \"col2\", \"col3\"]\n",
    "\n",
    "# new column names\n",
    "new_names = [\"new_col1\", \"new_col2\", \"new_col3\"]\n",
    "\n",
    "df.show()\n",
    "\n",
    "# renaming\n",
    "for old_name, new_name in zip(old_names, new_names):\n",
    "    df = df.withColumnRenamed(old_name, new_name)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df547a3",
   "metadata": {},
   "source": [
    "## 11. How to bin a numeric list to 10 groups of equal size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0e0c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "# Create a DataFrame with a single column \"values\" filled with random numbers\n",
    "num_items = 100\n",
    "df = spark.range(num_items).select(rand(seed=42).alias(\"values\"))\n",
    "\n",
    "df.show(5)\n",
    "# Define the bucket boundaries\n",
    "num_buckets = 10\n",
    "quantiles = df.stat.approxQuantile(\"values\", [i/num_buckets for i in range(num_buckets+1)], 0.01)\n",
    "\n",
    "# Create the Bucketizer\n",
    "bucketizer = Bucketizer(splits=quantiles, inputCol=\"values\", outputCol=\"buckets\")\n",
    "\n",
    "# Apply the Bucketizer\n",
    "df_buck = bucketizer.transform(df)\n",
    "\n",
    "#Frequency table\n",
    "df_buck.groupBy(\"buckets\").count().show()\n",
    "\n",
    "# Show the original and bucketed values\n",
    "df_buck.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6cb3c3",
   "metadata": {},
   "source": [
    "## 12. How to create contigency table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbd3ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example DataFrame\n",
    "data = [(\"A\", \"X\"), (\"A\", \"Y\"), (\"A\", \"X\"), (\"B\", \"Y\"), (\"B\", \"X\"), (\"C\", \"X\"), (\"C\", \"X\"), (\"C\", \"Y\")]\n",
    "df = spark.createDataFrame(data, [\"category1\", \"category2\"])\n",
    "\n",
    "df.show()\n",
    "# Frequency\n",
    "df.cube(\"category1\").count().show()\n",
    "\n",
    "# Contingency table\n",
    "df.crosstab('category1', 'category2').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a80ef3",
   "metadata": {},
   "source": [
    "## 13. How to find the numbers that are multiples of 3 from a column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f5b307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "\n",
    "# Generate a DataFrame with a single column \"id\" with 10 rows\n",
    "df = spark.range(10)\n",
    "\n",
    "# Generate a random float between 0 and 1, scale and shift it to get a random integer between 1 and 10\n",
    "df = df.withColumn(\"random\", ((rand(seed=42) * 10) + 1).cast(\"int\"))\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Assuming df is your DataFrame and \"your_column\" is the column with the numbers\n",
    "df = df.withColumn(\"is_multiple_of_3\", when(col(\"random\") % 3 == 0, 1).otherwise(0))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe9940e",
   "metadata": {},
   "source": [
    "## 14. How to extract items at given positions from a column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91179cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, monotonically_increasing_id\n",
    "\n",
    "# Generate a DataFrame with a single column \"id\" with 10 rows\n",
    "df = spark.range(10)\n",
    "\n",
    "# Generate a random float between 0 and 1, scale and shift it to get a random integer between 1 and 10\n",
    "df = df.withColumn(\"random\", ((rand(seed=42) * 10) + 1).cast(\"int\"))\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "pos = [0, 4, 8, 5]\n",
    "\n",
    "# Define window specification\n",
    "w = Window.orderBy(monotonically_increasing_id())\n",
    "\n",
    "# Add index\n",
    "df = df.withColumn(\"index\", row_number().over(w) - 1)\n",
    "\n",
    "df.show()\n",
    "\n",
    "# Filter the DataFrame based on the specified positions\n",
    "df_filtered = df.filter(df.index.isin(pos))\n",
    "\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b09190e",
   "metadata": {},
   "source": [
    "## 15. How to stack two DataFrames vertically ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbd9ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for region A\n",
    "df_A = spark.createDataFrame([(\"apple\", 3, 5), (\"banana\", 1, 10), (\"orange\", 2, 8)], [\"Name\", \"Col_1\", \"Col_2\"])\n",
    "df_A.show()\n",
    "\n",
    "# Create DataFrame for region B\n",
    "df_B = spark.createDataFrame([(\"apple\", 3, 5), (\"banana\", 1, 15), (\"grape\", 4, 6)], [\"Name\", \"Col_1\", \"Col_3\"])\n",
    "df_B.show()\n",
    "\n",
    "df_A.union(df_B).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebeeaec",
   "metadata": {},
   "source": [
    "## 16. How to compute the mean squared error on a truth and predicted columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73819924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume you have a DataFrame df with two columns \"actual\" and \"predicted\"\n",
    "# For the sake of example, we'll create a sample DataFrame\n",
    "data = [(1, 1), (2, 4), (3, 9), (4, 16), (5, 25)]\n",
    "df = spark.createDataFrame(data, [\"actual\", \"predicted\"])\n",
    "\n",
    "df.show()\n",
    "# Calculate the squared differences\n",
    "df = df.withColumn(\"squared_error\", pow((col(\"actual\") - col(\"predicted\")), 2))\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = df.agg({\"squared_error\": \"avg\"}).collect()[0][0]\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) = {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbef628",
   "metadata": {},
   "source": [
    "## 17. How to convert the first character of each element in a series to uppercase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d977d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import initcap\n",
    "# Suppose you have the following DataFrame\n",
    "data = [(\"john\",), (\"alice\",), (\"bob\",)]\n",
    "df = spark.createDataFrame(data, [\"name\"])\n",
    "\n",
    "df.show()\n",
    "# Convert the first character to uppercase\n",
    "df = df.withColumn(\"name\", initcap(df[\"name\"]))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198f2b2c",
   "metadata": {},
   "source": [
    "## 18. How to compute summary statistics for all columns in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ee6dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the sake of example, we'll create a sample DataFrame\n",
    "data = [('James', 34, 55000),\n",
    "('Michael', 30, 70000),\n",
    "('Robert', 37, 60000),\n",
    "('Maria', 29, 80000),\n",
    "('Jen', 32, 65000)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\" , \"salary\"])\n",
    "\n",
    "df.show()\n",
    "# Summary statistics\n",
    "summary = df.summary()\n",
    "\n",
    "# Show the summary statistics\n",
    "summary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816b382b",
   "metadata": {},
   "source": [
    "## 19. How to calculate the number of characters in each word in a column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a1a00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Suppose you have the following DataFrame\n",
    "data = [(\"john\",), (\"alice\",), (\"bob\",)]\n",
    "df = spark.createDataFrame(data, [\"name\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn('word_length', F.length(df.name))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4050a84d",
   "metadata": {},
   "source": [
    "## 20 How to compute difference of differences between consecutive numbers of a column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecd8005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# For the sake of example, we'll create a sample DataFrame\n",
    "data = [('James', 34, 55000),\n",
    "('Michael', 30, 70000),\n",
    "('Robert', 37, 60000),\n",
    "('Maria', 29, 80000),\n",
    "('Jen', 32, 65000)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\" , \"salary\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "# Define window specification\n",
    "df = df.withColumn(\"id\", F.monotonically_increasing_id())\n",
    "window = Window.orderBy(\"id\")\n",
    "\n",
    "# Generate the lag of the variable\n",
    "df = df.withColumn(\"prev_value\", F.lag(df.salary).over(window))\n",
    "\n",
    "# Compute the difference with lag\n",
    "df = df.withColumn(\"diff\", F.when(F.isnull(df.salary - df.prev_value), 0)\n",
    ".otherwise(df.salary - df.prev_value)).drop(\"id\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b510a467",
   "metadata": {},
   "source": [
    "## 21. How to get the day of month, week number, day of year and day of week from a date strings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bc2fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, dayofmonth, weekofyear, dayofyear, dayofweek\n",
    "# example data\n",
    "data = [(\"2023-05-18\",\"01 Jan 2010\",), (\"2023-12-31\", \"01 Jan 2010\",)]\n",
    "df = spark.createDataFrame(data, [\"date_str_1\", \"date_str_2\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "# Convert date string to date format\n",
    "df = df.withColumn(\"date_1\", to_date(df.date_str_1, 'yyyy-MM-dd'))\n",
    "df = df.withColumn(\"date_2\", to_date(df.date_str_2, 'dd MMM yyyy'))\n",
    "\n",
    "df = df.withColumn(\"day_of_month\", dayofmonth(df.date_1))\\\n",
    ".withColumn(\"week_number\", weekofyear(df.date_1))\\\n",
    ".withColumn(\"day_of_year\", dayofyear(df.date_1))\\\n",
    ".withColumn(\"day_of_week\", dayofweek(df.date_1))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1c2129",
   "metadata": {},
   "source": [
    "## 22. How to convert year-month string to dates corresponding to the 4th day of the month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f14a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, col\n",
    "# example dataframe\n",
    "df = spark.createDataFrame([('Jan 2010',), ('Feb 2011',), ('Mar 2012',)], ['MonthYear'])\n",
    "\n",
    "df.show()\n",
    "\n",
    "# convert YearMonth to date (default to first day of the month)\n",
    "df = df.withColumn('Date', expr(\"to_date(MonthYear, 'MMM yyyy')\"))\n",
    "\n",
    "df.show()\n",
    "\n",
    "# replace day with 4\n",
    "df = df.withColumn('Date', expr(\"date_add(date_sub(Date, day(Date) - 1), 3)\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040d9707",
   "metadata": {},
   "source": [
    "## 23 How to filter words that contain atleast 2 vowels from a series?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6a67d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, length, translate\n",
    "# example dataframe\n",
    "df = spark.createDataFrame([('Apple',), ('Orange',), ('Plan',) , ('Python',) , ('Money',)], ['Word'])\n",
    "\n",
    "df.show()\n",
    "\n",
    "# Filter words that contain at least 2 vowels\n",
    "df_filtered = df.where((length(col('Word')) - length(translate(col('Word'), 'AEIOUaeiou', ''))) >= 2)\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84ba8a9",
   "metadata": {},
   "source": [
    "## 24. How to filter valid emails from a list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ec1e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list\n",
    "data = ['buying books at amazom.com', 'rameses@egypt.com', 'matt@t.co', 'narendra@modi.com']\n",
    "\n",
    "# Convert the list to DataFrame\n",
    "df = spark.createDataFrame(data, \"string\")\n",
    "df.show(truncate =False)\n",
    "\n",
    "# Define a regular expression pattern for emails\n",
    "pattern = \"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$\"\n",
    "\n",
    "# Apply filter operation to keep only valid emails\n",
    "df_filtered = df.filter(F.col(\"value\").rlike(pattern))\n",
    "\n",
    "# Show the DataFrame\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e497bb5",
   "metadata": {},
   "source": [
    "## 25. How to Pivot PySpark DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c125f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data = [\n",
    "(2021, 1, \"US\", 5000),\n",
    "(2021, 1, \"EU\", 4000),\n",
    "(2021, 2, \"US\", 5500),\n",
    "(2021, 2, \"EU\", 4500),\n",
    "(2021, 3, \"US\", 6000),\n",
    "(2021, 3, \"EU\", 5000),\n",
    "(2021, 4, \"US\", 7000),\n",
    "(2021, 4, \"EU\", 6000),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"year\", \"quarter\", \"region\", \"revenue\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n",
    "# Execute the pivot operation\n",
    "pivot_df = df.groupBy(\"year\", \"quarter\").pivot(\"region\").sum(\"revenue\")\n",
    "\n",
    "pivot_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac661d0",
   "metadata": {},
   "source": [
    "## 26. How to get the mean of a variable grouped by another variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f59b581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "\n",
    "# Sample data\n",
    "data = [(\"1001\", \"Laptop\", 1000),\n",
    "(\"1002\", \"Mouse\", 50),\n",
    "(\"1003\", \"Laptop\", 1200),\n",
    "(\"1004\", \"Mouse\", 30),\n",
    "(\"1005\", \"Smartphone\", 700)]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"OrderID\", \"Product\", \"Price\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.show()\n",
    "\n",
    "\n",
    "# GroupBy and aggregate\n",
    "result = df.groupBy(\"Product\").agg(mean(\"Price\").alias(\"Total_Sales\"))\n",
    "\n",
    "# Show results\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd93dc2",
   "metadata": {},
   "source": [
    "## 27. How to compute the euclidean distance between two columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3b34ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Define your series\n",
    "data = [(1, 10), (2, 9), (3, 8), (4, 7), (5, 6), (6, 5), (7, 4), (8, 3), (9, 2), (10, 1)]\n",
    "\n",
    "# Convert list to DataFrame\n",
    "df = spark.createDataFrame(data, [\"series1\", \"series2\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "\n",
    "# Convert series to vectors\n",
    "vecAssembler = VectorAssembler(inputCols=[\"series1\", \"series2\"], outputCol=\"vectors\")\n",
    "df = vecAssembler.transform(df)\n",
    "\n",
    "# Calculate squared differences\n",
    "df = df.withColumn(\"squared_diff\", expr(\"POW(series1 - series2, 2)\"))\n",
    "\n",
    "# Sum squared differences and take square root\n",
    "df.agg(expr(\"SQRT(SUM(squared_diff))\").alias(\"euclidean_distance\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4848c6e9",
   "metadata": {},
   "source": [
    "## 28. How to replace missing spaces in a string with the least frequent character?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba13881",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, explode\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "from collections import Counter\n",
    "\n",
    "#Sample DataFrame\n",
    "df = spark.createDataFrame([('dbc deb abed gade',),], [\"string\"])\n",
    "df.show()\n",
    "\n",
    "\n",
    "def least_freq_char_replace_spaces(s):\n",
    "    counter = Counter(s.replace(\" \", \"\"))\n",
    "    least_freq_char = min(counter, key = counter.get)\n",
    "    return s.replace(' ', least_freq_char)\n",
    "\n",
    "udf_least_freq_char_replace_spaces = udf(least_freq_char_replace_spaces, StringType())\n",
    "\n",
    "df = spark.createDataFrame([('dbc deb abed gade',)], [\"string\"])\n",
    "df.withColumn('modified_string', udf_least_freq_char_replace_spaces(df['string'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1537cccd",
   "metadata": {},
   "source": [
    "## 29. How to create a TimeSeries starting ‘2000-01-01’ and 10 weekends (saturdays) after that having random numbers as values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefdb8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, explode, sequence, rand\n",
    "\n",
    "# Start date and end date (start + 10 weekends)\n",
    "start_date = '2000-01-01'\n",
    "end_date = '2000-03-04' # Calculated manually: 10 weekends (Saturdays) from start date\n",
    "\n",
    "# Create a DataFrame with one row containing a sequence from start_date to end_date with a 1 day step\n",
    "df = spark.range(1).select(\n",
    "explode(\n",
    "sequence(\n",
    "expr(f\"date '{start_date}'\"),\n",
    "expr(f\"date '{end_date}'\"),\n",
    "expr(\"interval 1 day\")\n",
    ")\n",
    ").alias(\"date\")\n",
    ")\n",
    "\n",
    "# Filter out the weekdays (retain weekends)\n",
    "df = df.filter(expr(\"dayofweek(date) = 7\")) # 7 corresponds to Saturday in Spark\n",
    "\n",
    "# Add the random numbers column\n",
    "#df = df.withColumn(\"random_numbers\", rand()*10)\n",
    "df = df.withColumn(\"random_numbers\", ((rand(seed=42) * 10) + 1).cast(\"int\"))\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88eaf98",
   "metadata": {},
   "source": [
    "## 30. How to get the nrows, ncolumns, datatype of a dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709cbee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/selva86/datasets/master/Churn_Modelling.csv\"\n",
    "\n",
    "spark.sparkContext.addFile(url)\n",
    "\n",
    "df = spark.read.csv(SparkFiles.get(\"Churn_Modelling.csv\"), header=True, inferSchema=True)\n",
    "\n",
    "df.show(5, truncate=False)\n",
    "\n",
    "# For number of rows\n",
    "nrows = df.count()\n",
    "print(\"Number of Rows: \", nrows)\n",
    "\n",
    "# For number of columns\n",
    "ncols = len(df.columns)\n",
    "print(\"Number of Columns: \", ncols)\n",
    "\n",
    "# For data types of each column\n",
    "datatypes = df.dtypes\n",
    "print(\"Data types: \", datatypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7978abc",
   "metadata": {},
   "source": [
    "## 31. How to rename a specific columns in a dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7613b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose you have the following DataFrame\n",
    "df = spark.createDataFrame([('Alice', 1, 30),('Bob', 2, 35)], [\"name\", \"age\", \"qty\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "# Rename lists for specific columns\n",
    "old_names = [\"qty\", \"age\"]\n",
    "new_names = [\"user_qty\", \"user_age\"]\n",
    "\n",
    "old_names = [\"qty\", \"age\"]\n",
    "new_names = [\"user_qty\", \"user_age\"]\n",
    "\n",
    "# You can then rename the columns like this:\n",
    "for old_name, new_name in zip(old_names, new_names):\n",
    "    df = df.withColumnRenamed(old_name, new_name)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e61fff",
   "metadata": {},
   "source": [
    "## 32. How to check if a dataframe has any missing values and count of missing values in each column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0c52fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "df = spark.createDataFrame([\n",
    "(\"A\", 1, None),\n",
    "(\"B\", None, \"123\" ),\n",
    "(\"B\", 3, \"456\"),\n",
    "(\"D\", None, None),\n",
    "], [\"Name\", \"Value\", \"id\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "\n",
    "missing = df.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns))\n",
    "has_missing = any(row.asDict().values() for row in missing.collect())\n",
    "print(has_missing)\n",
    "\n",
    "missing_count = missing.collect()[0].asDict()\n",
    "print(missing_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c2e079",
   "metadata": {},
   "source": [
    "## 33. How to replace missing values of multiple numeric columns with the mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce817ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "df = spark.createDataFrame([\n",
    "(\"A\", 1, None),\n",
    "(\"B\", None, 123 ),\n",
    "(\"B\", 3, 456),\n",
    "(\"D\", 6, None),\n",
    "], [\"Name\", \"var1\", \"var2\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "\n",
    "column_names = [\"var1\", \"var2\"]\n",
    "\n",
    "# Initialize the Imputer\n",
    "imputer = Imputer(inputCols= column_names, outputCols= column_names, strategy=\"mean\")\n",
    "\n",
    "# Fit the Imputer\n",
    "model = imputer.fit(df)\n",
    "\n",
    "#Transform the dataset\n",
    "imputed_df = model.transform(df)\n",
    "\n",
    "imputed_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2553e288",
   "metadata": {},
   "source": [
    "## 34. How to change the order of columns of a dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4975ba70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data = [(\"John\", \"Doe\", 30), (\"Jane\", \"Doe\", 25), (\"Alice\", \"Smith\", 22)]\n",
    "\n",
    "# Create DataFrame from the data\n",
    "df = spark.createDataFrame(data, [\"First_Name\", \"Last_Name\", \"Age\"])\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "new_order = [\"Age\", \"First_Name\", \"Last_Name\"]\n",
    "\n",
    "# Reorder the columns\n",
    "df = df.select(*new_order)\n",
    "\n",
    "# Show the DataFrame with reordered columns\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bc2f11",
   "metadata": {},
   "source": [
    "## 35. How to format or suppress scientific notations in a PySpark DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de8e273",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import format_number\n",
    "# Assuming you have a DataFrame df and the column you want to format is 'your_column'\n",
    "df = spark.createDataFrame([(1, 0.000000123), (2, 0.000023456), (3, 0.000345678)], [\"id\", \"your_column\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "\n",
    "# Determine the number of decimal places you want\n",
    "decimal_places = 10\n",
    "\n",
    "df = df.withColumn(\"your_column\", format_number(\"your_column\", decimal_places))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ca2e5d",
   "metadata": {},
   "source": [
    "## 36. How to format all the values in a dataframe as percentages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3034c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, col, lit\n",
    "\n",
    "# Sample data\n",
    "data = [(0.1, .08), (0.2, .06), (0.33, .02)]\n",
    "df = spark.createDataFrame(data, [\"numbers_1\", \"numbers_2\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "columns = [\"numbers_1\", \"numbers_2\"]\n",
    "for col_name in columns:\n",
    "    df = df.withColumn(col_name, concat((col(col_name) * 100).cast('decimal(10, 2)'), lit(\"%\")))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786f4195",
   "metadata": {},
   "source": [
    "## 37. How to filter every nth row in a dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99c7cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data = [(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 3), (\"Dave\", 4), (\"Eve\", 5),\n",
    "(\"Frank\", 6), (\"Grace\", 7), (\"Hannah\", 8), (\"Igor\", 9), (\"Jack\", 10)]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Number\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "# Define window\n",
    "window = Window.orderBy(monotonically_increasing_id())\n",
    "\n",
    "# Add row_number to DataFrame\n",
    "df = df.withColumn(\"rn\", row_number().over(window))\n",
    "\n",
    "n = 5 # filter every 5th row\n",
    "\n",
    "# Filter every nth row\n",
    "df = df.filter((df.rn % n) == 0)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0282170d",
   "metadata": {},
   "source": [
    "## 38 How to get the row number of the nth largest value in a column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932ff2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc, row_number\n",
    "\n",
    "# Sample Data\n",
    "data = [\n",
    "Row(id=1, column1=5),\n",
    "Row(id=2, column1=8),\n",
    "Row(id=3, column1=12),\n",
    "Row(id=4, column1=1),\n",
    "Row(id=5, column1=15),\n",
    "Row(id=6, column1=7),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()\n",
    "\n",
    "\n",
    "\n",
    "window = Window.orderBy(desc(\"column1\"))\n",
    "df = df.withColumn(\"row_number\", row_number().over(window))\n",
    "\n",
    "n = 3 # We're interested in the 3rd largest value.\n",
    "row = df.filter(df.row_number == n).first()\n",
    "\n",
    "if row:\n",
    "    print(\"Row number:\", row.row_number)\n",
    "    print(\"Column value:\", row.column1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d309f60",
   "metadata": {},
   "source": [
    "## 39. How to get the last n rows of a dataframe with row sum > 100?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730d0d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from functools import reduce\n",
    "# Sample data\n",
    "data = [(10, 25, 70),\n",
    "(40, 5, 20),\n",
    "(70, 80, 100),\n",
    "(10, 2, 60),\n",
    "(40, 50, 20)]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"col1\", \"col2\", \"col3\"])\n",
    "\n",
    "# Display original DataFrame\n",
    "df.show()\n",
    "\n",
    "# Add 'row_sum' column\n",
    "df = df.withColumn('row_sum', reduce(lambda a, b: a+b, [F.col(x) for x in df.columns]))\n",
    "\n",
    "# Display DataFrame with 'row_sum'\n",
    "df.show()\n",
    "\n",
    "# Filter rows where 'row_sum' > 100\n",
    "df = df.filter(F.col('row_sum') > 100)\n",
    "\n",
    "# Display filtered DataFrame\n",
    "df.show()\n",
    "\n",
    "# Add 'id' column\n",
    "df = df.withColumn('id', F.monotonically_increasing_id())\n",
    "\n",
    "# Get the last 2 rows\n",
    "df_last_2 = df.sort(F.desc('id')).limit(2)\n",
    "\n",
    "# Display the last 2 rows\n",
    "df_last_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d836b036",
   "metadata": {},
   "source": [
    "## 40. How to create a column containing the minimum by maximum of each row?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6440ebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, array\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# Sample Data\n",
    "data = [(1, 2, 3), (4, 5, 6), (7, 8, 9), (10, 11, 12)]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"col1\", \"col2\", \"col3\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "\n",
    "# Define UDF\n",
    "def min_max_ratio(row):\n",
    "    return float(min(row)) / max(row)\n",
    "\n",
    "min_max_ratio_udf = udf(min_max_ratio, FloatType())\n",
    "\n",
    "# Apply UDF to create new column\n",
    "df = df.withColumn('min_by_max', min_max_ratio_udf(array(df.columns)))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee11e4b7",
   "metadata": {},
   "source": [
    "## 41. How to create a column that contains the penultimate value in each row?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6bd5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "\n",
    "data = [(10, 20, 30),\n",
    "(40, 60, 50),\n",
    "(80, 70, 90)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Column1\", \"Column2\", \"Column3\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "\n",
    "# Define UDF to sort array in descending order\n",
    "sort_array_desc = F.udf(lambda arr: sorted(arr), ArrayType(IntegerType()))\n",
    "\n",
    "# Create array from columns, sort in descending order and get the penultimate value\n",
    "df = df.withColumn(\"row_as_array\", sort_array_desc(F.array(df.columns)))\n",
    "df = df.withColumn(\"Penultimate\", df['row_as_array'].getItem(1))\n",
    "df = df.drop('row_as_array')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a3ff46",
   "metadata": {},
   "source": [
    "## 42. How to normalize all columns in a dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e922ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# create a sample dataframe\n",
    "data = [(1, 2, 3),\n",
    "(2, 3, 4),\n",
    "(3, 4, 5),\n",
    "(4, 5, 6)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Col1\", \"Col2\", \"Col3\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "# define the list of columns to be normalized\n",
    "input_cols = [\"Col1\", \"Col2\", \"Col3\"]\n",
    "\n",
    "# initialize VectorAssembler with input and output column names\n",
    "assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
    "\n",
    "# transform the data\n",
    "df_assembled = assembler.transform(df)\n",
    "\n",
    "# initialize StandardScaler\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "\n",
    "# fit and transform the data\n",
    "scalerModel = scaler.fit(df_assembled)\n",
    "df_normalized = scalerModel.transform(df_assembled)\n",
    "\n",
    "# if you want to drop the original 'features' column\n",
    "df_normalized = df_normalized.drop('features')\n",
    "\n",
    "df_normalized.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab03738",
   "metadata": {},
   "source": [
    "## 43. How to get the positions where values of two columns match?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658e1165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create sample DataFrame\n",
    "data = [(\"John\", \"John\"), (\"Lily\", \"Lucy\"), (\"Sam\", \"Sam\"), (\"Lucy\", \"Lily\")]\n",
    "df = spark.createDataFrame(data, [\"Name1\", \"Name2\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "\n",
    "# Add new column Match to indicate if Name1 and Name2 match\n",
    "df = df.withColumn(\"Match\", when(col(\"Name1\") == col(\"Name2\"), True).otherwise(False))\n",
    "\n",
    "# Display DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f6f285",
   "metadata": {},
   "source": [
    "## 44. How to create lags and leads of a column by group in a dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb50a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lag, lead, to_date\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = [(\"2023-01-01\", \"Store1\", 100),\n",
    "(\"2023-01-02\", \"Store1\", 150),\n",
    "(\"2023-01-03\", \"Store1\", 200),\n",
    "(\"2023-01-04\", \"Store1\", 250),\n",
    "(\"2023-01-05\", \"Store1\", 300),\n",
    "(\"2023-01-01\", \"Store2\", 50),\n",
    "(\"2023-01-02\", \"Store2\", 60),\n",
    "(\"2023-01-03\", \"Store2\", 80),\n",
    "(\"2023-01-04\", \"Store2\", 90),\n",
    "(\"2023-01-05\", \"Store2\", 120)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Date\", \"Store\", \"Sales\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "# Convert the date from string to date type\n",
    "df = df.withColumn(\"Date\", to_date(df.Date, 'yyyy-MM-dd'))\n",
    "\n",
    "# Create a Window partitioned by Store, ordered by Date\n",
    "windowSpec = Window.partitionBy(\"Store\").orderBy(\"Date\")\n",
    "\n",
    "# Create lag and lead variables\n",
    "df = df.withColumn(\"Lag_Sales\", lag(df[\"Sales\"]).over(windowSpec))\n",
    "df = df.withColumn(\"Lead_Sales\", lead(df[\"Sales\"]).over(windowSpec))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fe0c2d",
   "metadata": {},
   "source": [
    "## 45. How to get the frequency of unique values in the entire dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca96a69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# Create a numeric DataFrame\n",
    "data = [(1, 2, 3),\n",
    "(2, 3, 4),\n",
    "(1, 2, 3),\n",
    "(4, 5, 6),\n",
    "(2, 3, 4)]\n",
    "df = spark.createDataFrame(data, [\"Column1\", \"Column2\", \"Column3\"])\n",
    "\n",
    "# Print DataFrame\n",
    "df.show()\n",
    "\n",
    "\n",
    "# get column names\n",
    "columns = df.columns\n",
    "\n",
    "# stack all columns into a single column\n",
    "df_single = None\n",
    "\n",
    "for c in columns:\n",
    "    if df_single is None:\n",
    "        df_single = df.select(col(c).alias(\"single_column\"))\n",
    "else:\n",
    "    df_single = df_single.union(df.select(col(c).alias(\"single_column\")))\n",
    "\n",
    "# generate frequency table\n",
    "frequency_table = df_single.groupBy(\"single_column\").count().orderBy('count', ascending=False)\n",
    "\n",
    "# show frequency table\n",
    "frequency_table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e234a1f",
   "metadata": {},
   "source": [
    "## 46. How to replace both the diagonals of dataframe with 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a22f7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, monotonically_increasing_id\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Create a numeric DataFrame\n",
    "data = [(1, 2, 3, 4),\n",
    "(2, 3, 4, 5),\n",
    "(1, 2, 3, 4),\n",
    "(4, 5, 6, 7)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"col_1\", \"col_2\", \"col_3\", \"col_4\"])\n",
    "\n",
    "# Print DataFrame\n",
    "df.show()\n",
    "\n",
    "\n",
    "# Define window specification\n",
    "w = Window.orderBy(monotonically_increasing_id())\n",
    "\n",
    "# Add index\n",
    "df = df.withColumn(\"id\", row_number().over(w) - 1)\n",
    "\n",
    "df = df.select([when(col(\"id\") == i, 0).otherwise(col(\"col_\"+str(i+1))).alias(\"col_\"+str(i+1)) for i in range(4)])\n",
    "\n",
    "# Create a reverse id column\n",
    "df = df.withColumn(\"id\", row_number().over(w) - 1)\n",
    "df = df.withColumn(\"id_2\", df.count() - 1 - df[\"id\"])\n",
    "\n",
    "df_with_diag_zero = df.select([when(col(\"id_2\") == i, 0).otherwise(col(\"col_\"+str(i+1))).alias(\"col_\"+str(i+1)) for i in range(4)])\n",
    "\n",
    "df_with_diag_zero.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836ac288",
   "metadata": {},
   "source": [
    "## 47. How to reverse the rows of a dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0441578b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, monotonically_increasing_id\n",
    "\n",
    "# Create a numeric DataFrame\n",
    "data = [(1, 2, 3, 4),\n",
    "(2, 3, 4, 5),\n",
    "(3, 4, 5, 6),\n",
    "(4, 5, 6, 7)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"col_1\", \"col_2\", \"col_3\", \"col_4\"])\n",
    "\n",
    "# Print DataFrame\n",
    "df.show()\n",
    "\n",
    "# Define window specification\n",
    "w = Window.orderBy(monotonically_increasing_id())\n",
    "\n",
    "# Add index\n",
    "df = df.withColumn(\"id\", row_number().over(w) - 1)\n",
    "\n",
    "df_2 = df.orderBy(\"id\", ascending=False).drop(\"id\")\n",
    "\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b730eb9a",
   "metadata": {},
   "source": [
    "## 48. How to create one-hot encodings of a categorical variable (dummy variables)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928812c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "data = [(\"A\", 10),(\"A\", 20),(\"B\", 30),(\"B\", 20),(\"B\", 30),(\"C\", 40),(\"C\", 10),(\"D\", 10)]\n",
    "columns = [\"Categories\", \"Value\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"Categories\", outputCol=\"Categories_Indexed\")\n",
    "indexerModel = indexer.fit(df)\n",
    "indexed_df = indexerModel.transform(df)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"Categories_Indexed\", outputCol=\"Categories_onehot\")\n",
    "encoded_df = encoder.fit(indexed_df).transform(indexed_df)\n",
    "encoded_df = encoded_df.drop(\"Categories_Indexed\")\n",
    "encoded_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0065a9fe",
   "metadata": {},
   "source": [
    "## 49. How to Pivot the dataframe (converting rows into columns) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23855351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data = [\n",
    "(2021, 1, \"US\", 5000),\n",
    "(2021, 1, \"EU\", 4000),\n",
    "(2021, 2, \"US\", 5500),\n",
    "(2021, 2, \"EU\", 4500),\n",
    "(2021, 3, \"US\", 6000),\n",
    "(2021, 3, \"EU\", 5000),\n",
    "(2021, 4, \"US\", 7000),\n",
    "(2021, 4, \"EU\", 6000),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"year\", \"quarter\", \"region\", \"revenue\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Execute the pivot operation\n",
    "pivot_df = df.groupBy(\"year\", \"quarter\").pivot(\"region\").sum(\"revenue\")\n",
    "\n",
    "pivot_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0debf1bf",
   "metadata": {},
   "source": [
    "## 50. How to UnPivot the dataframe (converting columns into rows) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588975d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "# Sample data\n",
    "data = [(2021, 2, 4500, 5500),\n",
    "(2021, 1, 4000, 5000),\n",
    "(2021, 3, 5000, 6000),\n",
    "(2021, 4, 6000, 7000)]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"year\", \"quarter\", \"EU\", \"US\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.show()\n",
    "\n",
    "unpivotExpr = \"stack(2, 'EU',EU, 'US', US) as (region,revenue)\"\n",
    "\n",
    "unPivotDF = pivot_df.select(\"year\",\"quarter\", expr(unpivotExpr)).where(\"revenue is not null\")\n",
    "\n",
    "unPivotDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee56c5b",
   "metadata": {},
   "source": [
    "## 51. How to impute missing values with Zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5be20f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose df is your DataFrame\n",
    "df = spark.createDataFrame([(1, None), (None, 2), (3, 4), (5, None)], [\"a\", \"b\"])\n",
    "\n",
    "df.show()\n",
    "df_imputed = df.fillna(0)\n",
    "\n",
    "df_imputed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1456d48d",
   "metadata": {},
   "source": [
    "## 52. How to identify continuous variables in a dataframe and create a list of those column names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cd5e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/selva86/datasets/master/Churn_Modelling_m.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "\n",
    "df = spark.read.csv(SparkFiles.get(\"Churn_Modelling_m.csv\"), header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "df.show(2, truncate=False)\n",
    "\n",
    "from pyspark.sql.types import IntegerType, StringType, NumericType\n",
    "from pyspark.sql.functions import approxCountDistinct\n",
    "\n",
    "def detect_continuous_variables(df, distinct_threshold):\n",
    "    continuous_columns = []\n",
    "    for column in df.columns:\n",
    "        dtype = df.schema[column].dataType\n",
    "        if isinstance(dtype, (IntegerType, NumericType)):\n",
    "            distinct_count = df.select(approxCountDistinct(column)).collect()[0][0]\n",
    "        if distinct_count > distinct_threshold:\n",
    "            continuous_columns.append(column)\n",
    "    return continuous_columns\n",
    "\n",
    "continuous_variables = detect_continuous_variables(df, 10)\n",
    "print(continuous_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe22e27b",
   "metadata": {},
   "source": [
    "## 53. How to calculate Mode of a PySpark DataFrame column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab94ffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# Create a sample DataFrame\n",
    "data = [(1, 2, 3), (2, 2, 3), (2, 2, 4), (1, 2, 3), (1, 1, 3)]\n",
    "columns = [\"col1\", \"col2\", \"col3\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.show()\n",
    "\n",
    "df_grouped = df.groupBy('col2').count()\n",
    "mode_df = df_grouped.orderBy(col('count').desc()).limit(1)\n",
    "\n",
    "mode_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1612b2ac",
   "metadata": {},
   "source": [
    "## 54. How to find installed location of Apache Spark and PySpark?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c948eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "print(findspark.find())\n",
    "\n",
    "import os\n",
    "import pyspark\n",
    "\n",
    "print(os.path.dirname(pyspark.__file__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6f6003",
   "metadata": {},
   "source": [
    "## 55. How to convert a column to lower case using UDF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887919cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Create a DataFrame to test\n",
    "data = [('John Doe', 'NEW YORK'),\n",
    "('Jane Doe', 'LOS ANGELES'),\n",
    "('Mike Johnson', 'CHICAGO'),\n",
    "('Sara Smith', 'SAN FRANCISCO')]\n",
    "\n",
    "df = spark.createDataFrame(data, ['Name', 'City'])\n",
    "\n",
    "df.show()\n",
    "\n",
    "\n",
    "# Define your UDF function\n",
    "def to_lower(s):\n",
    "    if s is not None:\n",
    "        return s.lower()\n",
    "\n",
    "# Convert your Python function to a Spark UDF\n",
    "udf_to_lower = udf(to_lower, StringType())\n",
    "\n",
    "# Apply your UDF to the DataFrame\n",
    "df = df.withColumn('City_lower', udf_to_lower(df['City']))\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee67903",
   "metadata": {},
   "source": [
    "## 56. How to convert PySpark data frame to pandas dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1647a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to test\n",
    "data = [('John Doe', 'NEW YORK'),\n",
    "('Jane Doe', 'LOS ANGELES'),\n",
    "('Mike Johnson', 'CHICAGO'),\n",
    "('Sara Smith', 'SAN FRANCISCO')]\n",
    "\n",
    "pysparkDF = spark.createDataFrame(data, ['Name', 'City'])\n",
    "\n",
    "pysparkDF.show()\n",
    "\n",
    "pandasDF = pysparkDF.toPandas()\n",
    "\n",
    "print(pandasDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34988da0",
   "metadata": {},
   "source": [
    "## 57. How to View PySpark Cluster Details?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28a626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffed92d",
   "metadata": {},
   "source": [
    "## 58. How to View PySpark Cluster Configuration Details?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db68a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all configurations\n",
    "for k,v in spark.sparkContext.getConf().getAll():\n",
    "    print(f\"{k} : {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d3be31",
   "metadata": {},
   "source": [
    "## 59. How to restrict the PySpark to use the number of cores in the system?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bb4a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.executor.cores\", \"2\") # set the number of cores you want here\n",
    "\n",
    "# Reuse existing SparkContext if present, otherwise create one with the given configuration\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afe934f",
   "metadata": {},
   "source": [
    "## 60. How to cache PySpark DataFrame or objects and delete cache?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada0e26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caching the DataFrame\n",
    "df.cache()\n",
    "\n",
    "# un-cache or unpersist data using the unpersist() method.\n",
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c8c809",
   "metadata": {},
   "source": [
    "## 61. How to Divide a PySpark DataFrame randomly in a given ratio (0.8, 0.2)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3deb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly split data (0.8, 0.2)\n",
    "train_data, test_data = pysparkDF.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fd1ad7",
   "metadata": {},
   "source": [
    "## 62. How to build logistic regression in PySpark?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31798267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "# Create a sample dataframe\n",
    "data = spark.createDataFrame([\n",
    "(0, 1.0, -1.0),\n",
    "(1, 2.0, 1.0),\n",
    "(1, 3.0, -2.0),\n",
    "(0, 4.0, 1.0),\n",
    "(1, 5.0, -3.0),\n",
    "(0, 6.0, 2.0),\n",
    "(1, 7.0, -1.0),\n",
    "(0, 8.0, 3.0),\n",
    "(1, 9.0, -2.0),\n",
    "(0, 10.0, 2.0),\n",
    "(1, 11.0, -3.0),\n",
    "(0, 12.0, 1.0),\n",
    "(1, 13.0, -1.0),\n",
    "(0, 14.0, 2.0),\n",
    "(1, 15.0, -2.0),\n",
    "(0, 16.0, 3.0),\n",
    "(1, 17.0, -3.0),\n",
    "(0, 18.0, 1.0),\n",
    "(1, 19.0, -1.0),\n",
    "(0, 20.0, 2.0)\n",
    "], [\"label\", \"feat1\", \"feat2\"])\n",
    "\n",
    "\n",
    "\n",
    "# convert the feature columns into a single vector column using VectorAssembler\n",
    "vecAssembler = VectorAssembler(inputCols=['feat1', 'feat2'], outputCol=\"features\")\n",
    "data = vecAssembler.transform(data)\n",
    "\n",
    "# fit the logistic regression model\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "lr_model = lr.fit(data)\n",
    "\n",
    "# look at the coefficients and intercept of the logistic regression model\n",
    "print(f\"Coefficients: {str(lr_model.coefficients)}\")\n",
    "print(f\"Intercept: {str(lr_model.intercept)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd257fed",
   "metadata": {},
   "source": [
    "## 63. How to convert the categorical string data into numerical data or index?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6c5d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "# Create a sample DataFrame\n",
    "data = [('cat',), ('dog',), ('mouse',), ('fish',), ('dog',), ('cat',), ('mouse',)]\n",
    "df = spark.createDataFrame(data, [\"animal\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "# Initialize a StringIndexer\n",
    "indexer = StringIndexer(inputCol='animal', outputCol='animalIndex')\n",
    "\n",
    "# Fit the indexer to the DataFrame and transform the data\n",
    "indexed = indexer.fit(df).transform(df)\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e87b183",
   "metadata": {},
   "source": [
    "## 64. How to calculate Correlation of two variables in a DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98ce84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample dataframe\n",
    "data = [Row(feature1=5, feature2=10, feature3=25),\n",
    "Row(feature1=6, feature2=15, feature3=35),\n",
    "Row(feature1=7, feature2=25, feature3=30),\n",
    "Row(feature1=8, feature2=20, feature3=60),\n",
    "Row(feature1=9, feature2=30, feature3=70)]\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "df.show()\n",
    "# Calculate correlation\n",
    "correlation = df.corr(\"feature1\", \"feature2\")\n",
    "\n",
    "print(\"Correlation between feature1 and feature2 :\", correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76094f2",
   "metadata": {},
   "source": [
    "## 65. How to calculate Correlation Matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaf334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "# Create a sample dataframe\n",
    "data = [Row(feature1=5, feature2=10, feature3=25),\n",
    "Row(feature1=6, feature2=15, feature3=35),\n",
    "Row(feature1=7, feature2=25, feature3=30),\n",
    "Row(feature1=8, feature2=20, feature3=60),\n",
    "Row(feature1=9, feature2=30, feature3=70)]\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "df.show()\n",
    "\n",
    "# Calculate Correlation Using Using MLlib\n",
    "\n",
    "# Assemble feature vector\n",
    "# Define the feature and label columns & Assemble the feature vector\n",
    "vector_assembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\", \"feature3\"], outputCol=\"features\")\n",
    "data_vector = vector_assembler.transform(df).select(\"features\")\n",
    "\n",
    "# Calculate correlation\n",
    "correlation_matrix = Correlation.corr(data_vector, \"features\").head()[0]\n",
    "\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e97281",
   "metadata": {},
   "source": [
    "## 66. How to calculate VIF (Variance Inflation Factor ) for set of variables in a DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3ce8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Create a sample dataframe\n",
    "data = [Row(feature1=5, feature2=10, feature3=25),\n",
    "Row(feature1=6, feature2=15, feature3=35),\n",
    "Row(feature1=7, feature2=25, feature3=30),\n",
    "Row(feature1=8, feature2=20, feature3=60),\n",
    "Row(feature1=9, feature2=30, feature3=70)]\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "df.show()\n",
    "\n",
    "\n",
    "def calculate_vif(data, features):\n",
    "    vif_dict = {}\n",
    "\n",
    "    for feature in features:\n",
    "        non_feature_cols = [col for col in features if col != feature]\n",
    "        assembler = VectorAssembler(inputCols=non_feature_cols, outputCol=\"features\")\n",
    "        lr = LinearRegression(featuresCol='features', labelCol=feature)\n",
    "\n",
    "        model = lr.fit(assembler.transform(data))\n",
    "        vif = 1 / (1 - model.summary.r2)\n",
    "\n",
    "        vif_dict[feature] = vif\n",
    "\n",
    "        return vif_dict\n",
    "\n",
    "features = ['feature1', 'feature2', 'feature3']\n",
    "vif_values = calculate_vif(df, features)\n",
    "\n",
    "for feature, vif in vif_values.items():\n",
    "    print(f'VIF for {feature}: {vif}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65ecfeb",
   "metadata": {},
   "source": [
    "## 67. How to perform Chi-Square test?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a61d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "# Create a sample dataframe\n",
    "data = [(1, 0, 0, 1, 1),\n",
    "(2, 0, 1, 0, 0),\n",
    "(3, 1, 0, 0, 0),\n",
    "(4, 0, 0, 1, 1),\n",
    "(5, 0, 1, 1, 0)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"id\", \"feature1\", \"feature2\", \"feature3\", \"label\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\", \"feature3\"], outputCol=\"features\")\n",
    "df = assembler.transform(df)\n",
    "\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "\n",
    "r = ChiSquareTest.test(df, \"features\", \"label\").head()\n",
    "print(\"pValues: \" + str(r.pValues))\n",
    "print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
    "print(\"statistics: \" + str(r.statistics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d85e0c",
   "metadata": {},
   "source": [
    "## 68. How to calculate the Standard Deviation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed9670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import stddev\n",
    "\n",
    "# Sample data\n",
    "data = [(\"James\", \"Sales\", 3000),\n",
    "(\"Michael\", \"Sales\", 4600),\n",
    "(\"Robert\", \"Sales\", 4100),\n",
    "(\"Maria\", \"Finance\", 3000),\n",
    "(\"James\", \"Sales\", 3000),\n",
    "(\"Scott\", \"Finance\", 3300),\n",
    "(\"Jen\", \"Finance\", 3900),\n",
    "(\"Jeff\", \"Marketing\", 3000),\n",
    "(\"Kumar\", \"Marketing\", 2000),\n",
    "(\"Saif\", \"Sales\", 4100)]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"Employee\", \"Department\", \"Salary\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "salary_stddev = df.select(stddev(\"Salary\").alias(\"stddev\"))\n",
    "\n",
    "salary_stddev.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bd1b1c",
   "metadata": {},
   "source": [
    "## 69. How to calculate missing value percentage in each column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d029f132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample dataframe\n",
    "data = [(\"John\", \"Doe\", None),\n",
    "(None, \"Smith\", \"New York\"),\n",
    "(\"Mike\", \"Smith\", None),\n",
    "(\"Anna\", \"Smith\", \"Boston\"),\n",
    "(None, None, None)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"FirstName\", \"LastName\", \"City\"])\n",
    "\n",
    "df.show()\n",
    "# Calculate the total number of rows in the dataframe\n",
    "total_rows = df.count()\n",
    "\n",
    "# For each column calculate the number of null values and then calculate the percentage\n",
    "for column in df.columns:\n",
    "    null_values = df.filter(df[column].isNull()).count()\n",
    "    missing_percentage = (null_values / total_rows) * 100\n",
    "    print(f\"Missing values in {column}: {missing_percentage}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50fee74",
   "metadata": {},
   "source": [
    "## 70. How to get the names of DataFrame objects that have been created in an environment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39553801",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_names = [name for name, obj in globals().items() if isinstance(obj, pyspark.sql.DataFrame)]\n",
    "\n",
    "for name in dataframe_names:\n",
    "    print(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
